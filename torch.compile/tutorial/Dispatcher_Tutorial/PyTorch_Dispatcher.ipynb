{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ref\n",
        "\n",
        "1. [如何 Dispatcher 里面注册算子](https://docs.pytorch.org/tutorials/advanced/dispatcher?utm_source=chatgpt.com)\n",
        "2.\n",
        "\n",
        "# PyTorch Dispatcher 入门到进阶（中文教程）\n",
        "\n",
        "本笔记面向 **初学者**，用可运行的小例子，带你循序渐进理解：\n",
        "\n",
        "1. Dispatcher 是什么、为什么需要它\n",
        "2. 如何**查看调度表（dispatch table）**，理解“一个算子在不同模式/设备下调用哪个实现”\n",
        "3. 用 `TorchDispatchMode` **拦截所有算子调用**，观察 dispatcher 实际路由\n",
        "4. 认识 `vmap`（向量化 map）：为什么在 vmap 中，算子的行为会切换到 **Batched** 语义\n",
        "5. 一些练习，帮助你加深理解\n",
        "\n",
        "> 提示：本笔记使用到的部分 API（如 `_dispatch_dump_table`、`TorchDispatchMode`）属于 **内部/私有** 或 **实验性** 接口，在不同 PyTorch 版本可能有差异。建议使用 PyTorch 2.1 及以上版本。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. 环境检查\n",
        "\n",
        "查看 PyTorch 版本、CUDA 是否可用、首个 GPU 名称（如有）。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch 版本: 2.7.1+cu126\n",
            "CUDA 可用: True\n",
            "GPU 设备: NVIDIA RTX A6000\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print('PyTorch 版本:', torch.__version__)\n",
        "print('CUDA 可用:', torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print('GPU 设备:', torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 直观理解 Dispatcher：为什么需要它？\n",
        "\n",
        "同一个算子（如 `add` 加法），在不同情境要走不同实现：\n",
        "\n",
        "- **设备**：CPU vs CUDA（GPU）\n",
        "- **是否需要梯度**：Autograd（反向传播）逻辑要插入/包装\n",
        "- **vmap 模式**：算子应具备“批处理（Batched）”语义\n",
        "- **tracing / 导出 / 混合精度** 等其他模式\n",
        "\n",
        "如果都塞进一个函数里用 `if/else` 判断，代码会变得不可维护。**Dispatcher** 通过“**dispatch key** → **kernel**”的映射表，在运行时根据张量属性与上下文，选择合适实现。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 查看算子的调度表（Dispatch Table）\n",
        "\n",
        "我们用 `_dispatch_dump_table` 打印 `aten::add` 与 `aten::mm` 的调度表，看看每个 **Dispatch Key** 对应哪个实现。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Undefined: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:7811 [default backend kernel]\n",
            "CPU: registered at /pytorch/build/aten/src/ATen/RegisterCPU_0.cpp:3455 [kernel]\n",
            "CUDA: registered at /pytorch/build/aten/src/ATen/RegisterCUDA_0.cpp:15875 [kernel]\n",
            "HIP: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:7811 [default backend kernel]\n",
            "MPS: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:7811 [default backend kernel]\n",
            "IPU: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:7811 [default backend kernel]\n",
            "XPU: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:7811 [default backend kernel]\n",
            "HPU: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:7811 [default backend kernel]\n",
            "VE: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:7811 [default backend kernel]\n",
            "MTIA: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:7811 [default backend kernel]\n",
            "PrivateUse1: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:7811 [default backend kernel]\n",
            "PrivateUse2: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:7811 [default backend kernel]\n",
            "PrivateUse3: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:7811 [default backend kernel]\n",
            "Meta: registered at /dev/null:488 [kernel]\n",
            "FPGA: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:7811 [default backend kernel]\n",
            "MAIA: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:7811 [default backend kernel]\n",
            "Vulkan: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:7811 [default backend kernel]\n",
            "Metal: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:7811 [default backend kernel]\n",
            "QuantizedCPU: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:7811 [default backend kernel]\n",
            "QuantizedCUDA: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:7811 [default backend kernel]\n",
            "QuantizedHIP: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:7811 [default backend kernel]\n",
            "QuantizedMPS: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:7811 [default backend kernel]\n",
            "QuantizedIPU: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:7811 [default backend kernel]\n",
            "QuantizedXPU: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:7811 [default backend kernel]\n",
            "QuantizedHPU: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:7811 [default backend kernel]\n",
            "QuantizedVE: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:7811 [default backend kernel]\n",
            "QuantizedMTIA: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:7811 [default backend kernel]\n",
            "QuantizedPrivateUse1: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:7811 [default backend kernel]\n",
            "QuantizedPrivateUse2: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:7811 [default backend kernel]\n",
            "QuantizedPrivateUse3: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:7811 [default backend kernel]\n",
            "QuantizedMeta: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:7811 [default backend kernel]\n",
            "CustomRNGKeyId: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:7811 [default backend kernel]\n",
            "MkldnnCPU: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:7811 [default backend kernel]\n",
            "SparseCPU: registered at /pytorch/build/aten/src/ATen/RegisterSparseCPU_0.cpp:1281 [kernel]\n",
            "SparseCUDA: registered at /pytorch/build/aten/src/ATen/RegisterSparseCUDA_0.cpp:1333 [kernel]\n",
            "SparseCsrCPU: registered at /pytorch/build/aten/src/ATen/RegisterSparseCsrCPU_0.cpp:1124 [kernel]\n",
            "SparseCsrCUDA: registered at /pytorch/build/aten/src/ATen/RegisterSparseCsrCUDA_0.cpp:1229 [kernel]\n",
            "SparseCsrHIP: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:7811 [default backend kernel]\n",
            "SparseCsrMPS: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:7811 [default backend kernel]\n",
            "SparseCsrIPU: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:7811 [default backend kernel]\n",
            "SparseCsrXPU: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:7811 [default backend kernel]\n",
            "SparseCsrHPU: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:7811 [default backend kernel]\n",
            "SparseCsrVE: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:7811 [default backend kernel]\n",
            "SparseCsrMTIA: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:7811 [default backend kernel]\n",
            "SparseCsrPrivateUse1: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:7811 [default backend kernel]\n",
            "SparseCsrPrivateUse2: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:7811 [default backend kernel]\n",
            "SparseCsrPrivateUse3: registered at /pytorch/build/aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional_0.cpp:7811 [default backend kernel]\n",
            "SparseCsrMeta: registered at /pytorch/build/aten/src/ATen/RegisterSparseCsrMeta_0.cpp:1089 [kernel]\n",
            "BackendSelect: fallthrough registered at /pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\n",
            "Python: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:194 [backend fallback]\n",
            "FuncTorchDynamicLayerBackMode: registered at /pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:479 [backend fallback]\n",
            "Functionalize: registered at /pytorch/aten/src/ATen/FunctionalizeFallbackKernel.cpp:349 [backend fallback]\n",
            "Named: fallthrough registered at /pytorch/aten/src/ATen/core/NamedRegistrations.cpp:11 [kernel]\n",
            "Conjugate: fallthrough registered at /pytorch/aten/src/ATen/ConjugateFallback.cpp:21 [kernel]\n",
            "Negative: registered at /pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\n",
            "ZeroTensor: registered at /pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\n",
            "ADInplaceOrView: fallthrough registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:100 [backend fallback]\n",
            "AutogradOther: registered at /pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:19588 [autograd kernel]\n",
            "AutogradCPU: registered at /pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:19588 [autograd kernel]\n",
            "AutogradCUDA: registered at /pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:19588 [autograd kernel]\n",
            "AutogradHIP: registered at /pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:19588 [autograd kernel]\n",
            "AutogradXLA: registered at /pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:19588 [autograd kernel]\n",
            "AutogradMPS: registered at /pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:19588 [autograd kernel]\n",
            "AutogradIPU: registered at /pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:19588 [autograd kernel]\n",
            "AutogradXPU: registered at /pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:19588 [autograd kernel]\n",
            "AutogradHPU: registered at /pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:19588 [autograd kernel]\n",
            "AutogradVE: registered at /pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:19588 [autograd kernel]\n",
            "AutogradLazy: registered at /pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:19588 [autograd kernel]\n",
            "AutogradMTIA: registered at /pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:19588 [autograd kernel]\n",
            "AutogradPrivateUse1: registered at /pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:19588 [autograd kernel]\n",
            "AutogradPrivateUse2: registered at /pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:19588 [autograd kernel]\n",
            "AutogradPrivateUse3: registered at /pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:19588 [autograd kernel]\n",
            "AutogradMeta: registered at /pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:19588 [autograd kernel]\n",
            "AutogradNestedTensor: registered at /pytorch/torch/csrc/autograd/generated/VariableType_3.cpp:19588 [autograd kernel]\n",
            "Tracer: registered at /pytorch/torch/csrc/autograd/generated/TraceType_3.cpp:14999 [kernel]\n",
            "AutocastCPU: registered at /pytorch/aten/src/ATen/autocast_mode.cpp:327 [kernel]\n",
            "AutocastMTIA: registered at /pytorch/aten/src/ATen/autocast_mode.cpp:470 [kernel]\n",
            "AutocastXPU: registered at /pytorch/aten/src/ATen/autocast_mode.cpp:508 [kernel]\n",
            "AutocastMPS: registered at /pytorch/aten/src/ATen/autocast_mode.cpp:213 [kernel]\n",
            "AutocastCUDA: registered at /pytorch/aten/src/ATen/autocast_mode.cpp:169 [kernel]\n",
            "FuncTorchBatched: registered at /pytorch/aten/src/ATen/functorch/BatchRulesLinearAlgebra.cpp:749 [kernel]\n",
            "BatchedNestedTensor: registered at /pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:758 [backend fallback]\n",
            "FuncTorchVmapMode: fallthrough registered at /pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:27 [backend fallback]\n",
            "Batched: registered at /pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:1079 [kernel]\n",
            "VmapMode: fallthrough registered at /pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n",
            "FuncTorchGradWrapper: registered at /pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:208 [backend fallback]\n",
            "PythonTLSSnapshot: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:202 [backend fallback]\n",
            "FuncTorchDynamicLayerFrontMode: registered at /pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:475 [backend fallback]\n",
            "PreDispatch: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:206 [backend fallback]\n",
            "PythonDispatcher: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:198 [backend fallback]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from torch import _C\n",
        "print(_C._dispatch_dump_table('aten::add'))\n",
        "print('\\n' + '='*80 + '\\n')\n",
        "print(_C._dispatch_dump_table('aten::mm'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 看懂输出的小贴士\n",
        "\n",
        "- **行**（或区块）代表不同的 **Dispatch Key**（如 `CPU`, `CUDA`, `AutogradCPU`, `AutogradCUDA`, `Batched`/`FuncTorchBatched` 等）\n",
        "- **目标**是该 key 下登记的 **kernel 实现**（可能是 C++/CUDA、或复合实现）\n",
        "- 运行时，dispatcher 会依据 **优先级** 和 **当前上下文** 选用正确的 key → kernel\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97c3d199",
      "metadata": {},
      "source": [
        "其中“vmap + dispatcher + batched 语义” 和你直接写 CUDA kernel／GPU 并行之间还有一些差别，它可以在高层（Python）写函数处理单样本，然后 vmap + dispatcher 自动把它“抬升”到批处理，无需自己管理并行细节。此外，vmap + dispatch key 的机制允许不同模式（Autograd、CUDA、CPU、Mixed Precision、Tracing 等）无缝组合，也允许控制流／索引／梯度等机制“不变或变化”。\n",
        "\n",
        "在 PyTorch 内部，大部分算子（operator）都是先写好了针对不同后端 /不同模式（比如 CPU、CUDA、Autograd、vmap 等模式）的实现（kernel），然后通过 dispatcher 来决定在运行时用哪个实现。这些实现／kernel 会被注册到 dispatcher 的不同 Dispatch Key 下。每个 key 表示一种“模式”：例如 CPU, CUDA, AutogradCUDA, Batched, AutocastCUDA 等。这样一个算子就不是一个函数，而是一个调度表，每一个 key 对应一个实现。\n",
        "\n",
        "当你在 Python 层调用算子（比如 x + y 或者 torch.add(x, y)），dispatcher 会做以下几步：\n",
        "\n",
        "收集所有参与输入的张量的 dispatch key（每个张量都有自己的 dispatch key 集）\n",
        "\n",
        "看当前线程／全局状态或模式，比如是否在 vmap 批量模式、是否 tracing、是否开启 autocast、是否需要 autograd 等\n",
        "\n",
        "组合这些 key，排除某些 key（如果有 exclude set），再按优先级排序\n",
        "\n",
        "从调度表里选第一个匹配的 kernel 执行\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 用 `TorchDispatchMode` 拦截算子调用\n",
        "\n",
        "我们用 `TorchDispatchMode` 写一个简单的 **日志模式**，打印每次算子调用的名称与张量信息。这样可以**直观看到** dispatcher 正在路由哪些 op。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DISPATCH] op=add.<bound method OpOverload.name of <OpOverload(op='aten.add', overload='Tensor')>> | args=[Tensor(device=cpu, dtype=torch.float32, shape=(3, 3), requires_grad=False), Tensor(device=cpu, dtype=torch.float32, shape=(3, 3), requires_grad=False)]\n",
            "[DISPATCH] op=mm.<bound method OpOverload.name of <OpOverload(op='aten.mm', overload='default')>> | args=[Tensor(device=cpu, dtype=torch.float32, shape=(3, 3), requires_grad=False), Tensor(device=cpu, dtype=torch.float32, shape=(3, 3), requires_grad=False)]\n",
            "结果形状: torch.Size([3, 3]) torch.Size([3, 3])\n"
          ]
        }
      ],
      "source": [
        "from torch.utils._python_dispatch import TorchDispatchMode\n",
        "import torch\n",
        "\n",
        "def _brief(x):\n",
        "    if isinstance(x, torch.Tensor):\n",
        "        return f\"Tensor(device={x.device}, dtype={x.dtype}, shape={tuple(x.shape)}, requires_grad={x.requires_grad})\"\n",
        "    return repr(x)\n",
        "\n",
        "class LoggingMode(TorchDispatchMode):\n",
        "    def __torch_dispatch__(self, func, types, args=(), kwargs=None):\n",
        "        kwargs = {} if kwargs is None else kwargs\n",
        "\n",
        "        # 1) 优先：完整名（e.g. 'aten.add.Tensor'）\n",
        "        op_full = None\n",
        "        name_attr = getattr(func, \"name\", None)\n",
        "        if callable(name_attr):\n",
        "            try:\n",
        "                op_full = name_attr()   # 关键：要调用！别忘了括号\n",
        "            except Exception:\n",
        "                op_full = None\n",
        "\n",
        "        # 2) 退化：基础名 + 重载（e.g. 'add.Tensor'）\n",
        "        if op_full is None:\n",
        "            base = getattr(getattr(func, \"overloadpacket\", None), \"__name__\", None)\n",
        "            overload = getattr(func, \"overload\", \"default\")\n",
        "            if base is not None:\n",
        "                op_full = f\"{base}.{overload}\"\n",
        "            else:\n",
        "                # 最保守的兜底\n",
        "                op_full = str(func)\n",
        "\n",
        "        arg_desc = \", \".join(_brief(a) for a in args)\n",
        "        print(f\"[DISPATCH] op={op_full} | args=[{arg_desc}]\")\n",
        "        return func(*args, **kwargs)\n",
        "\n",
        "# 小测\n",
        "x = torch.randn(3, 3)\n",
        "y = torch.randn(3, 3)\n",
        "with LoggingMode():\n",
        "    z = x + y\n",
        "    w = torch.mm(x, y)\n",
        "print(\"结果形状:\", z.shape, w.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "你应该能看到 `add` 与 `mm` 的 `[DISPATCH]` 日志。如果你在有 GPU 的机器上，把 `x = x.cuda(); y = y.cuda()`，再观察调用是否变化。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Autograd：带梯度的分发\n",
        "\n",
        "当张量 `requires_grad=True` 时，dispatcher 会让运算走 **Autograd** 的分支（先做记录/包装，再落到设备实现）。下面演示：\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = torch.randn(2, 3, requires_grad=True)\n",
        "y = torch.randn(2, 3)\n",
        "with LoggingMode():\n",
        "    out = (x + y).sum()\n",
        "    out.backward()  # 触发反向\n",
        "print('x.grad 形状:', x.grad.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "观察日志：你会看到前向的 `add/sum`，以及触发反向传播后，Autograd 路径会调度相应的反向算子。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. vmap：向量化语义如何影响分发\n",
        "\n",
        "在 `vmap` 中，同样的 per-sample 函数会被**向量化**到 batch 维。内部通常通过激活 **Batched** dispatch key，让算子采用 batched 语义（而不是让你手写 Python 循环）。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch import vmap\n",
        "\n",
        "def per_sample_fn(t):\n",
        "    return (t * t + torch.sin(t)).sum()\n",
        "\n",
        "batch = torch.randn(5, 4, requires_grad=True)\n",
        "\n",
        "# 5.1 手写循环（基准）\n",
        "loop_out = torch.stack([per_sample_fn(batch[i]) for i in range(batch.size(0))])\n",
        "\n",
        "# 5.2 vmap 版本（无显式 Python 循环）\n",
        "batched_fn = vmap(per_sample_fn)\n",
        "vmap_out = batched_fn(batch)\n",
        "\n",
        "print('loop_vs_vmap 是否一致:', torch.allclose(loop_out, vmap_out))\n",
        "print('输出形状:', loop_out.shape, vmap_out.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3 观察 vmap 下的调度日志\n",
        "\n",
        "把 `LoggingMode` 套在 vmap 调用外层，看看都触发了哪些 op。你会看到与 per-sample 函数里相同的算子，但它们会按 **Batched** 语义处理 batch 维。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with LoggingMode():\n",
        "    _ = batched_fn(batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. 小练习（巩固）\n",
        "\n",
        "1. 分别在 CPU 与 CUDA（如可用）上，比较 `aten::add` 的调度表输出有何差异。\n",
        "2. 把张量改为 `requires_grad=True`，观察日志里出现的 Autograd 相关算子。\n",
        "3. 修改 `per_sample_fn`，引入更多算子（如 `exp`、`log`、`matmul`），再用 vmap 对比“手写循环 vs vmap”的一致性与性能。\n",
        "4. 进一步尝试 `in_dims`/`out_dims` 的不同设置，理解 vmap 的输入/输出批维规则。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. 常见问答\n",
        "\n",
        "**Q1：为什么我本机没有 `TorchDispatchMode`？**\n",
        "\n",
        "- 你的 PyTorch 版本可能偏旧，或安装不完整。升级到新版本试试。\n",
        "\n",
        "**Q2：`_dispatch_dump_table` 打印不出来？**\n",
        "\n",
        "- 注意它是内部接口，某些版本可能隐藏/变更。也可以换打印别的算子（如 `aten::sin`）。\n",
        "\n",
        "**Q3：vmap 和 DataLoader 有什么区别？**\n",
        "\n",
        "- DataLoader 负责“**喂数据**”（批处理地加载样本），vmap 负责“**算子层面向量化**”（把对单样本的运算自动推广到批维，不必手写 for 循环）。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. 参考资料（建议阅读）\n",
        "\n",
        "- PyTorch Dispatcher 设计与背景（ezyang 博客）\n",
        "- PyTorch 官方 Dispatcher Walkthrough（GitHub Wiki）\n",
        "- `torch.vmap` 文档与教程\n",
        "- `TorchDispatchMode` / 扩展 PyTorch 笔记\n",
        "- 自定义算子 / `torch.library`（学习如何向调度表注册内核）\n",
        "\n",
        "（说明：以上链接请参见对话中的“参考链接”列表）\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
