{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 从入门到 vmap 大神：PyTorch vmap 教程（含手写 mini_vmap）\n",
        "更新时间：2025-09-20\n",
        "\n",
        "本笔记目标：\n",
        "- 直观理解 `vmap` 的语义与常用参数；\n",
        "- 观察一次 vmap 的执行过程；\n",
        "- **亲手实现一个可用的 `mini_vmap`**（支持 `in_dims/out_dims/randomness/chunk_size` 与 pytree）。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 快速上手\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch version: 2.7.1+cu126\n",
            "一致性: True\n",
            "输出形状: loop torch.Size([5]) | vmap torch.Size([5])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch\n",
        "print(\"torch version:\", torch.__version__)\n",
        "\n",
        "def per_sample_fn(t):\n",
        "    return (t * t + torch.sin(t)).sum()\n",
        "\n",
        "batch = torch.randn(5, 4, requires_grad=True)\n",
        "\n",
        "# 手写 for 循环（基准）\n",
        "loop_out = torch.stack([per_sample_fn(batch[i]) for i in range(batch.size(0))])\n",
        "\n",
        "# vmap 版本（无 Python 显式 for）\n",
        "batched_fn = torch.vmap(per_sample_fn)\n",
        "vmap_out = batched_fn(batch)\n",
        "\n",
        "print(\"一致性:\", torch.allclose(loop_out, vmap_out))\n",
        "print(\"输出形状: loop\", loop_out.shape, \"| vmap\", vmap_out.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 常用参数（in_dims / out_dims / randomness / chunk_size）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "out_dims=1 结果形状: torch.Size([5, 2])\n",
            "多输入 + None in_dim: torch.Size([4])\n",
            "randomness=error -> RuntimeError\n",
            "same 行相等? -> True\n",
            "different 行相等? -> False\n",
            "chunk 等价? -> True\n",
            "输出形状: torch.Size([64, 128])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# in_dims / out_dims\n",
        "x = torch.randn(2, 5)\n",
        "f = lambda z: z ** 2\n",
        "v1 = torch.vmap(f, out_dims=1)\n",
        "print(\"out_dims=1 结果形状:\", v1(x).shape)  # 期望 [5, 2]\n",
        "\n",
        "def dot_scale(a, b, scale: float = 1.0):\n",
        "    return torch.dot(a, b) * scale\n",
        "\n",
        "A = torch.randn(4, 8)  # [N, D]\n",
        "b = torch.randn(8)     # [D] 不带批维\n",
        "v2 = torch.vmap(dot_scale, in_dims=(0, None))\n",
        "print(\"多输入 + None in_dim:\", v2(A, b, scale=2.0).shape)  # [N]\n",
        "\n",
        "# randomness：演示 vmap 的随机策略\n",
        "def add_rand(z):\n",
        "    return z + torch.rand_like(z)\n",
        "\n",
        "Z = torch.zeros(3, 4)\n",
        "\n",
        "# 默认 randomness='error'：在 vmap 内调用随机算子会报错（演示）\n",
        "try:\n",
        "    torch.vmap(add_rand)(Z)\n",
        "except Exception as e:\n",
        "    print(\"randomness=error ->\", type(e).__name__)\n",
        "\n",
        "# randomness='same'：每个样本用同一份随机数\n",
        "same = torch.vmap(add_rand, randomness='same')(Z)\n",
        "print(\"same 行相等? ->\", torch.allclose(same[0], same[1]) and torch.allclose(same[1], same[2]))\n",
        "\n",
        "# randomness='different'：每个样本各用一份不同随机数\n",
        "diff = torch.vmap(add_rand, randomness='different')(Z)\n",
        "print(\"different 行相等? ->\", torch.allclose(diff[0], diff[1]))\n",
        "\n",
        "# ------------------------------\n",
        "# 方案 A：把 heavy 的随机权重移到 vmap 外（推荐）\n",
        "# ------------------------------\n",
        "\n",
        "# 原来：在 vmap 内生成随机 W，会触发 randomness='error' 报错\n",
        "# def heavy(op_in):\n",
        "#     W = torch.randn(op_in.size(-1), op_in.size(-1))\n",
        "#     return (op_in @ W).relu()\n",
        "\n",
        "# 修正：在 vmap 外只生成一次 W，然后当作参数传入\n",
        "def heavy_with_W(op_in, W_mat):\n",
        "    return (op_in @ W_mat).relu()\n",
        "\n",
        "big = torch.randn(64, 128)\n",
        "W = torch.randn(big.size(-1), big.size(-1))  # 在 vmap 外生成一次\n",
        "\n",
        "# vmap：第一个参数是批维（dim=0），第二个参数 W 不带批维\n",
        "full = torch.vmap(heavy_with_W, in_dims=(0, None))(big, W)\n",
        "\n",
        "# chunk_size 版本：语义应与 full 等价（同一份 W）\n",
        "chunked = torch.vmap(heavy_with_W, in_dims=(0, None), chunk_size=16)(big, W)\n",
        "\n",
        "print(\"chunk 等价? ->\", torch.allclose(full, chunked, atol=1e-5, rtol=1e-5))\n",
        "print(\"输出形状:\", full.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 观察一次分发（可选）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DISPATCH] op=aten::add.Tensor | args=[Tensor(device=cpu, dtype=torch.float32, shape=(3, 3, 3), requires_grad=False), Tensor(device=cpu, dtype=torch.float32, shape=(3, 3, 3), requires_grad=False)]\n",
            "[DISPATCH] op=aten::relu | args=[Tensor(device=cpu, dtype=torch.float32, shape=(3, 3, 3), requires_grad=False)]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from torch.utils._python_dispatch import TorchDispatchMode\n",
        "\n",
        "def _brief(x):\n",
        "    if isinstance(x, torch.Tensor):\n",
        "        return f\"Tensor(device={x.device}, dtype={x.dtype}, shape={tuple(x.shape)}, requires_grad={x.requires_grad})\"\n",
        "    return repr(x)\n",
        "\n",
        "class LoggingMode(TorchDispatchMode):\n",
        "    def __torch_dispatch__(self, func, types, args=(), kwargs=None):\n",
        "        kwargs = kwargs or {}\n",
        "        name_attr = getattr(func, \"name\", None)\n",
        "        op_full = name_attr() if callable(name_attr) else str(func)\n",
        "        arg_desc = \", \".join(_brief(a) for a in args)\n",
        "        print(f\"[DISPATCH] op={op_full} | args=[{arg_desc}]\")\n",
        "        return func(*args, **kwargs)\n",
        "\n",
        "def per_ex(a, b):\n",
        "    return (a + b).relu()\n",
        "\n",
        "A = torch.randn(3, 3, 3)\n",
        "B = torch.randn(3, 3, 3)\n",
        "\n",
        "with LoggingMode():\n",
        "    torch.vmap(per_ex)(A, B)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ce8b90b",
      "metadata": {},
      "source": [
        "# 4.vamp源码阅读\n",
        "\n",
        "这里会解读vmap.py，帮助读者更好的理解vmap的实现和机制\n",
        "\n",
        "## 辅助函数部分\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57714bb0",
      "metadata": {},
      "source": [
        "### `lazy_load_decompositions` 源码详注\n",
        "\n",
        "> 背景：在 `torch.func`/functorch 的 vmap/grad 等变换里，很多高阶算子需要“分解（decomposition）”为更基础、可变换/向量化的 ATen 原语。由于 TorchScript / 打包环境（`torch.package`）、以及 Python 3.11 的部分限制，这些分解并不是总能在进程启动时就安全注册，所以这里采用“**按需（lazy）加载**”的策略，只在需要且允许时注册到库中。参考：`torch.library` 官方文档、TorchScript 环境变量 `PYTORCH_JIT`、functorch 源码中对 vmap decomposition 的实现说明。  \n",
        "> 参阅：torch.library 文档、TorchScript/`PYTORCH_JIT`、functorch eager_transforms 源码、functorch → torch.func 迁移说明。  \n",
        "> 文档：[[torch.library]](https://docs.pytorch.org/docs/stable/library.html)｜[[TorchScript/ PYTORCH_JIT]](https://docs.pytorch.org/docs/stable/jit.html)｜[[env vars]](https://docs.pytorch.org/docs/stable/torch_environment_variables.html)｜[[functorch 源码片段]](https://docs.pytorch.org/functorch/1.13/_modules/functorch/_src/eager_transforms.html)｜[[functorch→torch.func]](https://docs.pytorch.org/docs/stable/func.migrating.html)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6226d5a8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# torch.package, Python 3.11, and torch.jit-less environments are unhappy with\n",
        "# decompositions. Only load them when needed if possible.\n",
        "def lazy_load_decompositions():\n",
        "    global DECOMPOSITIONS_LOADED\n",
        "    if DECOMPOSITIONS_LOADED:\n",
        "        return\n",
        "    # ↑ 全局幂等保护：如果已经装载过分解表（decomposition table），直接返回，避免重复注册。\n",
        "\n",
        "    with DECOMPOSITIONS_LOCK:\n",
        "        if DECOMPOSITIONS_LOADED:\n",
        "            return\n",
        "        # ↑ 二次检查 + 互斥锁：多线程/并发场景下防止重复装载。\n",
        "\n",
        "        # 只有在 JIT 打开 且 处于 debug 模式（__debug__ 为 True）时，才尝试注册 Python 侧分解。\n",
        "        # 否则直接标记为“已加载”并返回（不做任何注册），以规避：\n",
        "        # - 某些打包/环境下（如 torch.package、Python 3.11、禁用 JIT）对分解注册不友好的情况；\n",
        "        # - TorchScript 兼容性与类型系统限制（见下）。\n",
        "        if not (os.environ.get(\"PYTORCH_JIT\", \"1\") == \"1\" and __debug__):\n",
        "            DECOMPOSITIONS_LOADED = True\n",
        "            return\n",
        "        # 参考：PYTORCH_JIT 环境变量可禁用 JIT；禁用时一般不应执行基于 TorchScript 的注册路径。\n",
        "        # 文档：TorchScript / PYTORCH_JIT（官方）.\n",
        "\n",
        "        # 使用 torch.library 的“替代”注册方式把 Python decomposition 塞到表里。\n",
        "        # 直接用 _register_jit_decomposition 在某些算子（例如 aten::addr）上会失败：\n",
        "        #   原因是 TorchScript 生成的 Tensor 类型在一些分支上无法合并（union），导致类型系统报错。\n",
        "        # 因此这里通过 torch.library.Library(\"aten\", \"IMPL\", \"FuncTorchBatched\")\n",
        "        # 来向现有 aten 库注入 IMPL（实现）以覆盖/提供 decomposition。\n",
        "        global VMAP_DECOMPOSITIONS_LIB\n",
        "        VMAP_DECOMPOSITIONS_LIB = torch.library.Library(\n",
        "            \"aten\", \"IMPL\", \"FuncTorchBatched\"\n",
        "        )\n",
        "        # 参考：torch.library 允许对现有算子命名空间（如 \"aten\"）添加实现/别名/后端覆写等。\n",
        "        # 文档：torch.library（官方）。\n",
        "\n",
        "        from torch._decomp import decomposition_table\n",
        "        # ↑ PyTorch 维护的 Python 侧 decomposition 表（op → Python 实现函数），\n",
        "        #   常用于导出/编译/变换（如 torch.export.run_decompositions、AOTAutograd）等流程。\n",
        "        # 文档：export.run_decompositions / Core ATen decomposition（官方）。\n",
        "\n",
        "        def _register_python_decomposition_vmap(decomp):\n",
        "            # 如果该 op 在 decomposition_table 里有 Python 实现，则通过 torch.library 注入实现；\n",
        "            # 否则报错提示该 op 没有找到相应 decomposition。\n",
        "            if decomp in decomposition_table:\n",
        "                VMAP_DECOMPOSITIONS_LIB.impl(decomp, decomposition_table[decomp])\n",
        "            else:\n",
        "                raise RuntimeError(f\"could not find decomposition for {decomp}\")\n",
        "\n",
        "        # 下面把若干“梯度/损失类”算子和一个“线性代数”算子的 Python decomposition 挂进表里，\n",
        "        # 这样 vmap/grad 之类的变换在遇到它们时可以退化成更基础的 ATen 原语组合来执行。\n",
        "        _register_python_decomposition_vmap(torch.ops.aten.mse_loss_backward.default)\n",
        "        _register_python_decomposition_vmap(\n",
        "            torch.ops.aten.smooth_l1_loss_backward.default\n",
        "        )\n",
        "        _register_python_decomposition_vmap(torch.ops.aten.huber_loss_backward.default)\n",
        "        _register_python_decomposition_vmap(torch.ops.aten.nll_loss_forward.default)\n",
        "        _register_python_decomposition_vmap(torch.ops.aten.nll_loss2d_forward.default)\n",
        "        _register_python_decomposition_vmap(torch.ops.aten.nll_loss_backward.default)\n",
        "        _register_python_decomposition_vmap(torch.ops.aten.nll_loss2d_backward.default)\n",
        "        _register_python_decomposition_vmap(torch.ops.aten.addr.default)\n",
        "        # 注：functorch 源码注释里明确提到 “addr 在 _register_jit_decomposition 路径会出问题”，\n",
        "        #     因而采用 torch.library 的 IMPL 注入作为替代。\n",
        "        # 参阅：functorch eager_transforms 源码中的相同注释。\n",
        "\n",
        "        DECOMPOSITIONS_LOADED = True\n",
        "        # ↑ 成功注册后设置已加载标志，保证后续调用不会重复执行。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df8ee0d0",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "代码片断是 “注册 decomposition 到某个 DispatchKey 后端”的操作\n",
        "\n",
        "1. **为什么要 “lazy” 加载？**  \n",
        "   - 某些运行环境（如 `torch.package` 打包、Python 3.11、或“无 JIT”模式）对 TorchScript/分解注册不友好；  \n",
        "   - 过早注册可能触发类型系统/脚本化限制（例如 `aten::addr` 的类型并合问题），导致运行期错误；  \n",
        "   - 按需注册能减少冷启动开销，并把失败范围限定在真正需要用到分解的场景里。  \n",
        "   参考：TorchScript/`PYTORCH_JIT` 环境变量（官方），functorch 源码注释。  \n",
        "   资料：[[TorchScript/ PYTORCH_JIT]](https://docs.pytorch.org/docs/stable/jit.html)｜[[functorch 源码片段]](https://docs.pytorch.org/functorch/1.13/_modules/functorch/_src/eager_transforms.html)\n",
        "\n",
        "2. **`PYTORCH_JIT` 与条件分支**  \n",
        "   - 代码里要求 `PYTORCH_JIT == \"1\"` 且 `__debug__`（非 `-O` 运行）才进行注册；否则直接 “标记已加载并返回”，避免在不支持脚本化/不适合调试的环境里动分解表；  \n",
        "   - `PYTORCH_JIT=0` 会关闭所有脚本/trace 注解，让模型以 Python 态执行，便于调试，但也意味着很多依赖 TorchScript 的路径应当避免。  \n",
        "   资料：[[TorchScript 文档-禁用 JIT]](https://docs.pytorch.org/docs/stable/jit.html)｜[[环境变量总览]](https://docs.pytorch.org/docs/stable/torch_environment_variables.html)\n",
        "\n",
        "3. **为何用 `torch.library.Library(\"aten\", \"IMPL\", \"...\")`？**  \n",
        "   - `torch.library` 提供对算子库的“扩展/覆写”入口（创建自定义算子、为既有算子增加后端实现/别名等）；  \n",
        "   - 设置 `\"aten\"` 命名空间 + `\"IMPL\"` 表示给现有 ATen 算子提供实现层面的注册；  \n",
        "   - 这里用 `\"FuncTorchBatched\"` 作为后端/命名标签，承载 vmap/分解语义下的实现。  \n",
        "   资料：[[torch.library 官方]](https://docs.pytorch.org/docs/stable/library.html)\n",
        "\n",
        "4. **`decomposition_table` 是什么？**  \n",
        "   - Python 侧的“算子分解表”（op → Python 实现函数），广泛用于 `torch.export.run_decompositions`、AOTAutograd/`torch.compile` 及相关 passes，把复杂算子**替换**为核心 ATen 原语组合（Core ATen set），便于后端/编译器处理；  \n",
        "   - “Core ATen” 是一组**不会再继续分解**的基础算子集合；默认的 decompositions 会把高阶 op 分解到这套核心集合里。  \n",
        "   资料：[[export.run_decompositions]](https://docs.pytorch.org/docs/stable/export.html)｜[[Core ATen 定义]](https://docs.pytorch.org/executorch/stable/ir-ops-set-definition.html)｜（中文）[[export 教程（含 Core ATen 描述）]](https://pytorch-cn.com/tutorials/intermediate/torch_export_tutorial.html)\n",
        "\n",
        "5. **为什么特别点名 `aten::addr`？**  \n",
        "   - functorch 源码注释里指出 `_register_jit_decomposition` 在 `addr` 上会失败（TorchScript 类型系统无法对该算子的 Tensor 类型做 union），因此用 `torch.library` 的 IMPL 注入规避；  \n",
        "   - 这类“绕过 JIT 的 Python 侧注册”思路常见于 vmap/grad 的特殊链路。  \n",
        "   资料：[[functorch 源码片段]](https://docs.pytorch.org/functorch/1.13/_modules/functorch/_src/eager_transforms.html)\n",
        "\n",
        "6. **和 `torch.func` / functorch 的关系**  \n",
        "   - functorch 已经并入 PyTorch 核心库（`torch.func`），vmap/grad 等变换的实现依赖分解/包装（BatchedTensor）等机制；  \n",
        "   - 迁移指引详见官方说明。  \n",
        "   资料：[[functorch→torch.func 迁移]](https://docs.pytorch.org/docs/stable/func.migrating.html)\n",
        "\n",
        "7. **关于decomposition_table**\n",
        "   - `decomposition_table`(由`torch._decomp`维护) 是 PyTorch 内部用来存储「operator 分解（decomposition）」映射的一个结构，它把算子（operator）映射到一个 Python 实现函数，用于将一个算子拆解成一组更基础／更底层算子的组合。在很多变换／导出／编译流程中，这样的分解机制非常重要。这个表格（通常是一个字典／映射）里的键 (key) 是某些 ATen operator（torch.ops.aten.xxx），值 (value) 是对应一个 Python 函数／callable，这个函数能实现在更基础／更核心算子上的等价操作。\n",
        "\n",
        "---\n",
        "\n",
        "#### 小结\n",
        "\n",
        "- 该函数通过**幂等+加锁**保障只注册一次；  \n",
        "- 仅在 **JIT 开启且 debug 模式** 下注册 Python decompositions，其他环境直接跳过；  \n",
        "- 使用 `torch.library` 的 **IMPL** 路径把 `decomposition_table` 中已存在的分解实现“挂到” `aten` 算子上（尤其为了解决 `addr` 等在 TorchScript 路径中的类型限制问题）；  \n",
        "- 这让 vmap/grad/compile/export 等变换在遇到这些 op 时，能退化为“核心 ATen”组合，获得更好的兼容性与可编译性。  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b28c045",
      "metadata": {},
      "source": [
        "## 核心impl实现"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e865f5b3",
      "metadata": {},
      "outputs": [],
      "source": [
        "def vmap_impl(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs):\n",
        "    # 1) 惰性加载一些“分解规则”（decompositions），\n",
        "    #    供后续把复杂算子拆成更基础、可自动向量化的原语用。\n",
        "    #    好处：避免模块级 import 时就做重活儿，延迟到真正调用 vmap 时再加载。\n",
        "    lazy_load_decompositions()\n",
        "\n",
        "    # 2) 预检查 out_dims 的形状/类型是否和 func 的输出结构匹配：\n",
        "    #    - 允许 int 或“PyTree of int”（比如与多输出结构同构的树）。\n",
        "    #    - 不匹配会尽早报错，避免跑到一半才发现维度对不齐。\n",
        "    _check_out_dims_is_int_or_int_pytree(out_dims, func)\n",
        "\n",
        "    # 3) 解析输入批维并把实参拍平：\n",
        "    #    - 根据 in_dims（可为 int / None / PyTree of int/None），\n",
        "    #      找到每个输入张量上“要被 vmap 的那一维”（通常是 batch 维）。\n",
        "    #    - 统一拍平成 flat_args（方便后续统一处理），\n",
        "    #      并记录 flat_in_dims 和 args_spec（原 PyTree 结构的规格信息）。\n",
        "    #    - 同时确定 batch_size（即被映射的维度长度），用于后续切分或校验。\n",
        "    batch_size, flat_in_dims, flat_args, args_spec = _process_batched_inputs(\n",
        "        in_dims, args, func\n",
        "    )\n",
        "\n",
        "    # 4) 如果指定了 chunk_size，则走“分块 vmap”路径：\n",
        "    #    背景：当 batch 很大时直接向量化可能占用内存大或不够稳，\n",
        "    #    把 batch 维按 chunk_size 切成若干小块，逐块做 vmap，再把结果拼起来。\n",
        "    if chunk_size is not None:\n",
        "        # 4.1) 依据 flat_in_dims 在 batch 维上对 flat_args 做切片，得到每块的实参列表。\n",
        "        chunks_flat_args = _get_chunked_inputs(\n",
        "            flat_args, flat_in_dims, batch_size, chunk_size\n",
        "        )\n",
        "        # 4.2) 逐块执行 vmap（内部会把 func 应用到每一块的样本上），\n",
        "        #      并在 out_dims 指定的位置拼出结果的批维。\n",
        "        #      注意 randomness 的含义：\n",
        "        #      - 'error'：块内/块间如遇随机算子直接报错（默认行为）；\n",
        "        #      - 'same'：各样本共享相同随机性；\n",
        "        #      - 'different'：各样本使用独立随机性。\n",
        "        #      该开关只影响 PyTorch 自身的随机算子，不影响 Python/numpy 的随机。参见官方说明。\n",
        "        #      参见：torch.func.vmap(randomness=...) 文档。\n",
        "        #      https://docs.pytorch.org/docs/stable/generated/torch.func.vmap.html\n",
        "        return _chunked_vmap(\n",
        "            func,\n",
        "            flat_in_dims,\n",
        "            chunks_flat_args,\n",
        "            args_spec,\n",
        "            out_dims,\n",
        "            randomness,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "    # 5) 否则，走“整批 vmap”路径：\n",
        "    #    - 一次性对 batch 维做自动向量化，不切块。\n",
        "    #    - _flat_vmap 内部会完成：\n",
        "    #        a) 把需要做 vmap 的输入包装成带批维的“BatchedTensor”，\n",
        "    #        b) 调用 func，让其内部的张量运算在 BatchedTensor 语义下执行“批版算子”，\n",
        "    #        c) 按 out_dims 还原输出结构并在指定位置插入批维。\n",
        "    #    - 对随机策略 randomness 的处理同上。\n",
        "    return _flat_vmap(\n",
        "        func,\n",
        "        batch_size,\n",
        "        flat_in_dims,\n",
        "        flat_args,\n",
        "        args_spec,\n",
        "        out_dims,\n",
        "        randomness,\n",
        "        **kwargs,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cb699f1",
      "metadata": {},
      "source": [
        "## in_dims / out_dims / randomness / chunk_size 解释\n",
        "\n",
        "### 什么是 `in_dims`\n",
        "\n",
        "- `in_dims`：告诉 `vmap` “每个输入的哪个维度是要被批处理/映射（batch-map）过的维度”  \n",
        "- 类型可以是：\n",
        "  - `int`：对于所有输入张量，默认在这个维度被批处理（通常是 0）  \n",
        "  - `None`：表示该输入 **不随 batch 变化**，即这个输入没有被 `vmap` 映射维度  \n",
        "  - 与输入结构同构（PyTree）的嵌套结构（例如 list/tuple/dict），每个输入位置对应一个 `int` 或 `None`  \n",
        "- 默认值：`0`\n",
        "\n",
        "### 什么是 `out_dims`\n",
        "\n",
        "- `out_dims`：告诉 `vmap` “输出里新增的批维（映射维）应该放到输出张量的哪个维度位置”  \n",
        "- 类型可以是：\n",
        "  - `int`：对所有输出张量都在这个维度插入 batch 维  \n",
        "  - 与输出结构同构的嵌套结构／PyTree，如果函数有多个输出，每个输出对应一个位置 `int`  \n",
        "- 默认值：`0`，即输出的第一个维度是批次维。\n",
        "\n",
        "### `randomness` 的策略\n",
        "\n",
        "- `randomness` 决定 “PyTorch 随机算子”（例如 `torch.rand_like`／`torch.randn` 等）在批处理／vmap 中的行为  \n",
        "- 三种可选值：\n",
        "\n",
        "  1. `'error'`  \n",
        "     如果函数体中使用了 PyTorch 的随机生成功能，会报错。默认是这个模式。\n",
        "\n",
        "  2. `'same'`  \n",
        "     批次中的所有样本共享同一次随机性，例如同一个随机种子／同一个随机结果。用在你希望 batch 内部行为是一致随机的时候。\n",
        "\n",
        "  3. `'different'`  \n",
        "     为每个样本提供独立的随机性，即每个样本像独立调用随机函数那样，结果不同。用在你希望 batch 内每个元素随机行为不同的时候。\n",
        "\n",
        "- 注意事项：\n",
        "\n",
        "  - 这个 `randomness` 设定只影响 **PyTorch 的随机算子**，不影响 `Python random` 模块或 `numpy.random` 等。\n",
        "\n",
        "### `chunk_size` 的作用\n",
        "\n",
        "- 当批量（batch）很大时，直接把所有样本一次性向量化可能带来以下问题：\n",
        "\n",
        "  - 内存开销太高  \n",
        "  - 某些算子在大 batch 数下可能不稳定／效率低\n",
        "\n",
        "- `chunk_size` 提供了一种折中的方式：把数据按 batch 维度切成若干块，每块大小为 `chunk_size`，对每块分别 vmap，然后把所有块的结果按 `out_dims` 指定位置 **拼接** 回来。\n",
        "\n",
        "- 特别地：\n",
        "\n",
        "  - 如果 `chunk_size = 1`，等价于手写的 `for` 循环逐样本处理（batch size 1 每次）  \n",
        "  - 如果 `chunk_size = None`（默认），则一次性处理全部 batch 样本，不分块。\n",
        "\n",
        "---\n",
        "\n",
        "### 内部函数大致作用\n",
        "\n",
        "下面是一些内部函数／步骤的作用，助于理解 `vmap_impl` 在 `in_dims / out_dims / randomness / chunk_size` 参数控制下是如何工作的：\n",
        "\n",
        "| 内部函数 | 功能／做什么 |\n",
        "|---|---------------|\n",
        "| `lazy_load_decompositions()` | 延迟加载 “分解规则”（decompositions），供后续把复杂算子拆为基础的可批／可向量化处理方式用。 |\n",
        "| `_check_out_dims_is_int_or_int_pytree(out_dims, func)` | 校验 `out_dims` 是一个合法的对象：要么是 `int`，要么是一个与 `func` 输出结构同构的 PyTree，其中每个叶子是 `int`。不合法就报错。 |\n",
        "| `_process_batched_inputs(in_dims, args, func)` | 分析输入参数 `args`：<br> • 根据 `in_dims` 找出每个输入张量在哪个维度有 batch mapping；<br> • 拍平成扁平列表 `flat_args`，与之对应的 `flat_in_dims`；<br> • 保存输入的树型结构规格 `args_spec`（以便输出或结果重建结构）；<br> • 找到 `batch_size`（被映射的维度的大小）。 |\n",
        "| `_get_chunked_inputs(flat_args, flat_in_dims, batch_size, chunk_size)` | 按 `chunk_size` 把 `flat_args` 在 batch 维上切成块，得到每个块的参数子集，用于后续按块处理。 |\n",
        "| `_chunked_vmap(func, flat_in_dims, chunks_flat_args, args_spec, out_dims, randomness, **kwargs)` | 对每一块执行类似全量 vmap 的流程，再把每块的输出在 `out_dims` 的位置合并／拼接起来，形成与一次性 vmap 等效的结果（但内存要求可能更小、更加可控）。 |\n",
        "| `_flat_vmap(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)` | 主干路径：一次性把所有样本做批处理（不分块）。<br>• 把输入包装成具有 batch 维（按照 `flat_in_dims`）的 BatchedTensor（或等价机制），使 func 内部操作能在这些含有 batch 的张量上进行；<br>• func 执行；<br>• 输出中插入 batch 维度到 `out_dims` 指定的位置；<br>• 按输出结构重组返回。 |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b0937bf",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 手写 `mini_vmap`（教学版）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mini_vmap vs torch.vmap: True\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from torch.utils._pytree import tree_flatten, tree_unflatten\n",
        "\n",
        "class MiniError(Exception): \n",
        "    pass\n",
        "\n",
        "def _normalize_in_dims(in_dims, flat_len):\n",
        "    if isinstance(in_dims, (int, type(None))):\n",
        "        if flat_len != 1:\n",
        "            raise MiniError(\"in_dims 是标量，但有多个输入叶子；请传结构化 in_dims\")\n",
        "        return (in_dims,)\n",
        "    if isinstance(in_dims, (tuple, list)):\n",
        "        if len(in_dims) != flat_len:\n",
        "            raise MiniError(f\"in_dims 长度 {len(in_dims)} 与输入叶子数 {flat_len} 不符\")\n",
        "        return tuple(in_dims)\n",
        "    raise MiniError(\"in_dims 必须是 int/None 或 tuple/list\")\n",
        "\n",
        "def _infer_batch_size(flat_args, flat_in_dims):\n",
        "    bs = None\n",
        "    for a, d in zip(flat_args, flat_in_dims):\n",
        "        if d is None:\n",
        "            continue\n",
        "        if not isinstance(a, torch.Tensor):\n",
        "            raise MiniError(\"带 batch 维的输入必须是 Tensor\")\n",
        "        size_d = a.size(d if d >= 0 else a.dim() + d)\n",
        "        if bs is None:\n",
        "            bs = size_d\n",
        "        elif bs != size_d:\n",
        "            raise MiniError(f\"批大小不一致：之前 {bs}，当前 {size_d}\")\n",
        "    if bs is None:\n",
        "        raise MiniError(\"无法推断批大小：没有任何输入带 batch 维\")\n",
        "    return bs\n",
        "\n",
        "def _select_along(x: torch.Tensor, dim: int, i: int):\n",
        "    if dim < 0: \n",
        "        dim = x.dim() + dim\n",
        "    return x.select(dim, i)\n",
        "\n",
        "def _stack_outputs(per_item_outs, out_dims):\n",
        "    flat0, spec = tree_flatten(per_item_outs[0])\n",
        "    flat_items = []\n",
        "    for o in per_item_outs:\n",
        "        f, s = tree_flatten(o)\n",
        "        if s != spec:\n",
        "            raise MiniError(\"输出结构在批内不一致\")\n",
        "        flat_items.append(f)\n",
        "    leaves_per_slot = list(zip(*flat_items))\n",
        "    if isinstance(out_dims, int):\n",
        "        out_dims = (out_dims,) * len(leaves_per_slot)\n",
        "    elif isinstance(out_dims, (tuple, list)):\n",
        "        if len(out_dims) != len(leaves_per_slot):\n",
        "            raise MiniError(\"out_dims 长度与输出叶子数不匹配\")\n",
        "        out_dims = tuple(out_dims)\n",
        "    else:\n",
        "        raise MiniError(\"out_dims 必须是 int 或 tuple/list[int]\")\n",
        "    stacked_leaves = []\n",
        "    for leaves, od in zip(leaves_per_slot, out_dims):\n",
        "        if not all(isinstance(t, torch.Tensor) for t in leaves):\n",
        "            raise MiniError(\"演示实现仅支持 Tensor 叶子\")\n",
        "        stacked = torch.stack(leaves, dim=0)\n",
        "        _od = od if od >= 0 else stacked.dim() + od\n",
        "        if _od != 0:\n",
        "            perm = [p for p in range(stacked.dim()) if p != 0]\n",
        "            perm.insert(_od, 0)\n",
        "            stacked = stacked.permute(perm)\n",
        "        stacked_leaves.append(stacked)\n",
        "    return tree_unflatten(stacked_leaves, spec)\n",
        "\n",
        "def _apply_randomness(mode, i, base_seed):\n",
        "    if mode == \"error\":\n",
        "        return\n",
        "    elif mode == \"same\":\n",
        "        torch.manual_seed(base_seed)\n",
        "    elif mode == \"different\":\n",
        "        torch.manual_seed(base_seed + i)\n",
        "    else:\n",
        "        raise MiniError(\"randomness 仅支持 'error'|'same'|'different'\")\n",
        "\n",
        "def _chunk_slices(total, chunk_size):\n",
        "    k = chunk_size\n",
        "    i = 0\n",
        "    while i < total:\n",
        "        j = min(i + k, total)\n",
        "        yield i, j\n",
        "        i = j\n",
        "\n",
        "def mini_vmap(func, in_dims=0, out_dims=0, *, randomness=\"error\", chunk_size=None):\n",
        "    def wrapped(*args, **kwargs):\n",
        "        flat_args, spec = tree_flatten(args)\n",
        "        # 1) 解析 in_dims，推断 batch 大小\n",
        "        flat_in_dims = _normalize_in_dims(in_dims, len(flat_args))\n",
        "        B = _infer_batch_size(flat_args, flat_in_dims)\n",
        "        base_seed = int(torch.seed())\n",
        "        # 2) 分块执行（可选）\n",
        "        chunks = [(0, B)] if chunk_size is None else list(_chunk_slices(B, int(chunk_size)))\n",
        "        outs = []\n",
        "        for (lo, hi) in chunks:\n",
        "            for i in range(lo, hi):\n",
        "                _apply_randomness(randomness, i, base_seed)\n",
        "                # 3) 沿 in_dims 切片、调用 func\n",
        "                sliced = []\n",
        "                for a, d in zip(flat_args, flat_in_dims):\n",
        "                    if d is None:\n",
        "                        sliced.append(a)\n",
        "                    else:\n",
        "                        if not isinstance(a, torch.Tensor):\n",
        "                            raise MiniError(\"非 Tensor 不可带 batch 维\")\n",
        "                        sliced.append(_select_along(a, d, i))\n",
        "                call_args = tree_unflatten(sliced, spec)\n",
        "                outs.append(func(*call_args, **kwargs))\n",
        "        # 4) 聚合到 out_dims\n",
        "        return _stack_outputs(outs, out_dims)\n",
        "    return wrapped\n",
        "\n",
        "# 对比官方行为（若数值差异大则抛错）\n",
        "g = lambda t: (t*t + torch.sin(t)).sum()\n",
        "X = torch.randn(7, 4)\n",
        "ans_official = torch.vmap(g)(X)\n",
        "ans_mine = mini_vmap(g)(X)\n",
        "print(\"mini_vmap vs torch.vmap:\", torch.allclose(ans_official, ans_mine))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. 小测试与练习\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mini_vmap 基本测试通过 ✅\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def assert_close(x, y, tol=1e-6):\n",
        "    if not torch.allclose(x, y, atol=tol, rtol=tol):\n",
        "        raise AssertionError(\"Mismatch\")\n",
        "\n",
        "# 多输入 + None in_dim\n",
        "def ds(a, b): return torch.dot(a, b)\n",
        "A = torch.randn(8, 3); b = torch.randn(3)\n",
        "assert_close(torch.vmap(ds, in_dims=(0, None))(A, b), mini_vmap(ds, in_dims=(0, None))(A, b))\n",
        "\n",
        "# 结构化输出（两路输出）\n",
        "def two_out(x): return (x.sum(), x.mean())\n",
        "T = torch.randn(4, 10)\n",
        "o1 = torch.vmap(two_out)(T)\n",
        "o2 = mini_vmap(two_out)(T)\n",
        "assert_close(o1[0], o2[0]); assert_close(o1[1], o2[1])\n",
        "\n",
        "print(\"mini_vmap 基本测试通过 ✅\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
